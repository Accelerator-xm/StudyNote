# 理解ai

## 从函数到神经网络

### 函数

**函数**: 这个世界逻辑和知识都可以用函数来表示 "functions describe the world!"

#### 符号主义
**符号主义**: 人工智能早期的思路
现实世界 -> 符号化x -> 设置好规则(函数) -> 运算出结果y -> 解释现实世界
如: 输入直角边边长ab -> 根据勾股定理 -> 计算出斜边边长


#### 联结主义
**问题**: 不知道符号间的规律
很多东西不知道怎么写成明确的函数 （人太菜了导致的）
如: 识别猫和狗, 可以一眼看出, 但是无法写出一个函数识别

**联结主义**: 猜和简化问题
找不到精确函数，就找结果上大差不差的近似函数
不严谨但效果就是好

### 神经网络

#### 激活函数

**激活函数**: 线性 -> 非线性
线性函数 f(x)=wx+b 不能很好的拟合数据, 数据的趋势是曲线
可以在线性函数外套一个非线性运算, 如 sin(wx+b) 、 e^(wx+b)
非线性函数 f(x)=g(wx+b) 逼近能力更强

**扩展**: 套娃
多个特征：输入可以是多个变量 f(x1,x2) = g(w1·x1+w2·x2+b)
曲线不够拟合: 套多层激活函数 f(x1,x2) = g( w3·g(w1·x1+w2·x2+b) + b2 )
以此类推, 可以构造非常复杂的非线性函数, 理论上逼近任意函连续函数

#### 神经网络

y1 = g(w1·x1+w2·x2+b)
y = g( w3·g( y1 ) + b2 )

**神经网络**: 将x1, x2, y1, y 看成节点, 构造关系图，结构类似于生物神经的传到结构, 节点为神经元
<div align="center">
<img src="img/neural_network.png" alt="神经网络" width="400">
</div>

**前向传播**: 分步计算
输入层(x1,x2) -> 隐藏层(y1) -> 输出层(y)


## 神经网络参数

### 损失函数

**任务**: 拟合的好
对于函数 f(x) = wx+b
根据一组组(x1, y1), (x2, y2)...的值, 求出 w, b 要求计算的估计值y_test接近y
即: 误差 |y-y_test| 小

**损失函数**: 反应整体数据误差的函数
如: 对每组数据的误差就和 L = sum( |y-f(x)| )

**经典损失函数**:
均方误差: L = sum( (y-f(x))^2 )/N
为了消除绝对值不平滑、分类讨论等问题, 可以用平方消除绝对值, 还可以放大误差大的数据的影响
为了消除样本数量大小的影响, 可以取平均


### 线性回归

**线性回归**: 寻找线性函数来拟合xy之间的关系

**求解**: 极值点, 偏导为0
<div align="center">
<img src="img/2_1.png" alt="问题提取" width="400">
</div>

**举例**: 简化为 y=wx, 结果是y=x
<div align="center">
<img src="img/2_2.png" alt="样例数据" width="400">
<img src="img/2_3.png" alt="损失函数" width="400">
<img src="img/2_4.png" alt="求解损失函数最小" width="400">
</div>

### 梯度下降

**问题**: 神经网络有线性函数和非线性激活函数套娃出来的, 很难求偏导为0的解

**解决**: 试
先初始化系数, 然后慢慢尝试变化系数, 让误差减小

**梯度下降**: 沿着偏导的反方向(损失函数值减小)变化系数
<div align="center">
<img src="img/2_6.png" alt="梯度下降理解" width="400">
</div>

**学习率**: 控制变化的大小
<div align="center">
<img src="img/2_5.png" alt="梯度下降公式" width="400">
</div>

**链式法则**: 复合函数求导
<div align="center">
<img src="img/2_7.png" alt="链式法则" width="400">
</div>

**反向传播**: 先计算外层偏导进行更新, 在计算内层

**一次训练**: 
y_test = g( w2·g( w1·x+b1 )+b2 )
L = (y-y_test)^2
- 初始化 w1, b1, w2, b2
- 学习率 k
- 前向传播: x -> a -> y_test -> L
- 链式求导
    - L对y_test求偏导: -2y_test = m1
    - y_test对b2偏导: g'(w2·a + b2) = m2
    - y_test对w2偏导: a·g'(w2·a + b2) = m3
    - y_test对a求偏导: w2·g'(w2·a + b2) = m4
    - a对b1求偏导: g'(w1·a + b1) = m5
    - a对w1求偏导: x·g'(w1·a + b1) = m6
- 梯度变化
    - b2 = b2 - k1·m1·m2
    - w2 = w2 - k2·m1·m3
    - b1 = b1 - k3·m1·m4·m5
    - w1 = w1 - k4·m1·m4·m6


## 模型训练

### 模型能力

**目标**: 数据拟合的好, 损失函数较小
<div align="center">
<img src="img/3_1.png" alt="拟合对比1" width="500">
</div>

**泛化能力**: 在没见过的数据上的预测能力

### 过拟合

**提问**: 损失函数越小越好吗
<div align="center">
<img src="img/3_2.png" alt="拟合对比2" width="500">
</div>

**过拟合**: 训练集上好, 但预测数据不好
单从预测值与真实值误差来看, 右图更好
但直觉会感觉左图更好, 在没见过的数据上效果更好
<div align="center">
<img src="img/3_3.png" alt="拟合对比3" width="500">
</div>

**原因**: 想复杂了
原本数据是很简单的规律, 但是模型太复杂了, 吧噪声和随机波动也学会了

**解决**: 降低模型复杂度, 增加数据量

### 数据增强

**数据增强**: 在原有数据中创造更多数据

**方法**: 对图像选择、反转、裁剪、噪声...
<div align="center">
<img src="img/3_4.png" alt="数据增强" width="400">
</div>

**鲁棒性**: 受输入细微变化的影响
通过数据增强, 可以让输入数据细微变化时, 输出效果依然很好

### 训练优化

防止过拟合, 最简单粗暴的方式是提前终止训练, 差不多行了, 不太严谨

**正则化**: 给损失函数添加惩罚项, 抑制野蛮增长
当系数调整后带来的坏处(惩罚项变大)大于益处(损失函数减小), 就拒绝这次系数变化
把 损失函数+惩罚项 看成新的损失函数, 使新损失函数减小
- **惩罚项**:
    - L1正则化: sum( |w| ), L1范数
    - L2正则化: sum( (w)^2 ), L2范数
- **正则化系数**: 控制惩罚项影响大小

**超参数**: 控制参数的参数, 如学习率、正则化系数

**Dropout**: 每次训练随机丢弃一些训练数据, 减小个别数据的影响

- **训练问题**: 
    - **梯度消失**: 网络越深, 梯度反向传播时越来越小(小数越乘越小), 参数更新困难
    - **梯度爆炸**: 梯度数值越来越大, 参数调整幅度失去控制
    - **收敛速度过慢**: 陷入局部最优或者来回震荡
    - **计算开销过大**: 数据规模量太大, 每次完整的前向传播和反向传播非常耗时
- **优化方案**: 
    - **梯度裁剪**: 防止梯度更新过大
    - **残差网络**: 防止深层网络的梯度裁剪
    - **权重初始化/输入数据归一化**: 梯度分布更平滑
    - **动量法/RMSProp/Adam**: 优化器加速收敛
    - **mini-batch**: 数据分割成小批次, 降低单次计算开销


## 从矩阵到CNN

### 矩阵

**问题**: 当网络结构复杂时, 公式会很繁杂
**解决**: 引入矩阵简化公式
<div align="center">
<img src="img/4_1.png" alt="矩阵形式1" width="400">
<img src="img/4_2.png" alt="矩阵形式2" width="300">
<img src="img/4_3.png" alt="矩阵形式3" width="400">
</div>

**优点**: 矩阵运算可以从分利用GPU并行计算的特性, 加速训练和推理

### 卷积神经网络CNN
**问题**: 不能很好的理解图片的局部模式
对于30*30的图像, 输入就有900个节点, 假设下一层有1000个节点, 就有90万个参数, 而且仅仅是把图片平铺展开, 无法保留像素间的空间关系, 图片的细微变化可能使所有神经元和原来完全不同

**卷积核**: 一个固定的矩阵
在传统的图像处理领域, 该矩阵是已知的, 针对模糊、浮雕、轮廓、锐化等效果有不同的结果
在深度学习领域需要通过训练获得

- **卷积运算**:
    - 从原来图像中取出一小块, 如3*3的矩阵, 数值就是图片灰度值
    - 将该矩阵与卷积核进行运算: 对应位置相乘在求和
    - 遍历运算整个原图片, 形成一个新图像
<div align="center">
<img src="img/4_6.png" alt="卷积运算" width="400">
</div>


**卷积神经网络**:将一个全连接层替换成卷积层
减少参数数量、捕捉局部特征
公式上将矩阵乘法改成卷积
<div align="center">
<img src="img/4_4.png" alt="卷积神经网络1" width="400">
</div>

**池化层**: 对卷积层后的特征图像降维, 减少计算量, 保留主要特征
<div align="center">
<img src="img/4_5.png" alt="卷积神经网络2" width="400">
</div>


## 从词嵌入到RNN

### 词嵌入
**问题**: 计算机识别不了文字

**解决**: 编码
将文字转换成计算机能识别的数字, 一一对应

**编码方式**:
- 一个数字代表一个词: 
    - 维度太低, 相当于一维向量
    - 数字表示本身对语言理解没有任何意义, 无法衡量词与词之间的相关性

- one-hot独热编码: 准备超级大的向量, 每个词对应的向量只有一个位置是1
    - 维度太高, 非常稀疏
    - 每个向量之间正交, 无法找到词与词之间的相关性
    - 每个位置看成特征的话, 每个特征只能表示是或否

- word embedding**词嵌入**: 
    - 维度不高不低, 每个位置依然理解为某个特征, 训练出来的
    - 通过向量之间的**点积**和**余弦相似度**表示向量之间的相关性, 对应两个词之间的相关性
<div align="center">
<img src="img/5_1.png" alt="词向量" width="400">
</div>

**嵌入矩阵**: 将所有的词向量, 组成一个大矩阵
矩阵每一列为一个词向量, 该矩阵由深度学习的方法训练出来(如word2vec)
<div align="center">
<img src="img/5_2.png" alt="嵌入矩阵" width="400">
</div>

**潜空间**: 每个向量的维度非常高, 向量所在空间的维度非常高, 该空间为潜空间


### 循环神经网络RNN

**问题**: 
假设一句话有5个词, 每个词向量维度为300
对于神经网络, 输入层就有1500个节点
- 缺点:
    - 输入太大, 且每句话词语的数量长短不一, 输入不固定
    - 无法体现词之间的先后顺序

**解决**: 一个词一个词计算, 前一个结果影响后一个结果

- **RNN运算**: 
    - 第一个词计算隐藏状态 h1 = g(w1·x1 + b1)
    - 再计算出第一个输出 y1 = g(w3·h1 + b2)
    - 第二个词的隐藏状态受第一个隐藏状态的影响 h2 = g(w1·x2 + w2·h1 + b1)
    - 再计算第二个输出 y2 = g(w3·h2 + b2)
    - 以此类推
<div align="center">
<img src="img/5_3.png" alt="循环神经网络计算过程" width="400">
<img src="img/5_4.png" alt="循环神经网络1" width="400">
<img src="img/5_5.png" alt="循环神经网络2" width="400">
<img src="img/5_6.png" alt="循环神经网络展开理解" width="400">
</div>

- **RNN缺点**:
    - 无法捕捉长期依赖: 信息会随着时间步增多而逐渐丢失, 有些语句恰恰是很远的地方起到了关键作用
    - 无法并行计算: RNN必须按顺序计算
- **一些解决方法**: GRU和LSTM改进
    - 仍然是按照时间步顺序传递
    - 只能缓解不能根治

