{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925a4e76",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "深度学习框架通过自动计算导数，即**自动微分(automatic differentiation)**加快求导\n",
    "\n",
    "根据模型系统会构建**计算图(computational graph)**，跟踪哪些数据通过哪些操作组合产生输出\n",
    "\n",
    "自动微分能够**反向传播(backpropagate)**梯度：跟踪计算图，填充每个参数的偏导数\n",
    "\n",
    "## 简单例子\n",
    "\n",
    "<img src=\"img/1-5/1.png\" alt=\"例子\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7852088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a7fc1",
   "metadata": {},
   "source": [
    "计算y关于x的梯度之前需要一个地方存储梯度\n",
    "不会每次对一个参数求导时都分配新内存，因为经常会成千上万次地更新相同的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106dc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)  # 等价x = torch.arange(4.0, requires_grad=True)\n",
    "x.grad # 默认为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6b2718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算y\n",
    "\n",
    "y = 2 * torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde2915f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过调用反向传播函数自动计算y关于每个分量的梯度\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e35f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据上一章梯度的性质可以的到y的梯度为4x\n",
    "# 对比可知计算正确\n",
    "x.grad == 4*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e1417",
   "metadata": {},
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵\n",
    "对于高阶和高维的y和x，求导的结果可以是一个高阶张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06cfe348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非标量调用backward需要传入一个gradient参数，指定微分函数关于self的梯度\n",
    "# 本例求偏导数之和，传递1是合适的\n",
    "x.grad.zero_()  # 清空梯度\n",
    "y = x * x\n",
    "y.sum().backward() # 等价 y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd7b55",
   "metadata": {},
   "source": [
    "gradient参数的作用：**权重向量**，指定y中每个分量的权重\n",
    "  - [1, 1, 1]：f(y) = 1*y1 + 1*y2 + 1*y3 = sum(y)\n",
    "  - [2, 3, 4]：f(y) = 2*y1 + 3*y2 + 4*y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4ed785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4., 12., 24.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x*x\n",
    "y.backward(torch.tensor([1,2,3,4])) \n",
    "# 1*y1 + 2*y2 + 3*y3 + 4*y4\n",
    "# 梯度为 [2*x1, 4*x2, 6*x3, 8*x4]\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1464549",
   "metadata": {},
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望将某些计算移动到记录的计算图之外\n",
    "\n",
    "例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用\n",
    "\n",
    "可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb749255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "# 计算z=u*x关于x的偏导数\n",
    "# u被作为常数，不是计算z=x*x*x关于x的偏导数\n",
    "y = x*x\n",
    "u = y.detach()\n",
    "z = u*x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271a515",
   "metadata": {},
   "source": [
    "由于记录了y的计算结果，可以在y上调用反向传播，得到y关于x的导数，即2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a8b7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93954dd4",
   "metadata": {},
   "source": [
    "## python控制流的梯度计算\n",
    "\n",
    "自动微分的好处：即使构建函数的计算图需要通过控制流(条件、循环、函数调用)，仍然可以计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5bbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = 2*a\n",
    "    while b.norm() < 1000:\n",
    "        b = b*2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100*b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32ea8",
   "metadata": {},
   "source": [
    "分析f(a)：该函数时分段线性的，对于任何a，存在常量使得f(a)=ka，梯度为k，即d/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a7aadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
